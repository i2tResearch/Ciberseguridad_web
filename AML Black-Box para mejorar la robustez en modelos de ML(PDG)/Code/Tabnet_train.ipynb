{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "route = './datasets/'\n",
    "amount = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Importar datos benignos #\n",
    "carpeta_transformados_benign = route +'TransformadosBenign'\n",
    "\n",
    "# Obtener la lista de archivos pickle en la carpeta\n",
    "pickle_files = [f for f in os.listdir(carpeta_transformados_benign) if f.endswith('.pickle')]\n",
    "\n",
    "# Crear una lista vacía para almacenar los dataframes cargados\n",
    "df_list = []\n",
    "\n",
    "# Cargar cada archivo pickle en la lista\n",
    "for file in pickle_files:\n",
    "    if len(df_list) != amount:\n",
    "        with open(os.path.join(carpeta_transformados_benign, file), 'rb') as f:\n",
    "            df = pickle.load(f)\n",
    "            df_list.append(df)\n",
    "\n",
    "# Combinar los dataframes cargados en un solo dataframe\n",
    "df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>udps.n_bytes_per_packet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[88, 157, 254, 116, 4, 103, 81, 12, 21, 243, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[120, 131, 191, 232, 32, 48, 89, 92, 52, 223,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[60, 139, 25, 116, 70, 94, 194, 11, 248, 250,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[120, 248, 86, 132, 67, 44, 33, 52, 96, 194, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             udps.n_bytes_per_packet\n",
       "0  [[88, 157, 254, 116, 4, 103, 81, 12, 21, 243, ...\n",
       "1  [[120, 131, 191, 232, 32, 48, 89, 92, 52, 223,...\n",
       "2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "3  [[60, 139, 25, 116, 70, 94, 194, 11, 248, 250,...\n",
       "4  [[120, 248, 86, 132, 67, 44, 33, 52, 96, 194, ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114554"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Importar datos malignos #\n",
    "carpeta_transformados_malware = route + 'TransformadosMalware'\n",
    "\n",
    "# Obtener la lista de archivos pickle en la carpeta\n",
    "pickle_files = [f for f in os.listdir(carpeta_transformados_malware) if f.endswith('.pickle')]\n",
    "\n",
    "# Crear una lista vacía para almacenar los dataframes cargados\n",
    "df_doh_list = []\n",
    "\n",
    "# Cargar cada archivo pickle en la lista\n",
    "for file in pickle_files:\n",
    "    if len(df_doh_list) != amount:\n",
    "        with open(os.path.join(carpeta_transformados_malware, file), 'rb') as f:\n",
    "            df_doh = pickle.load(f)\n",
    "            df_doh_list.append(df_doh)\n",
    "\n",
    "# Combinar los dataframes cargados en un solo dataframe\n",
    "df_doh = pd.concat(df_doh_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign data shape: (114554, 1)\n",
      "Malware data shape: (63700, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Benign data shape: \" + str(df.shape))\n",
    "print(\"Malware data shape: \" + str(df_doh.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut data\n",
    "df_small = df.iloc[:10000]\n",
    "df_doh_small = df_doh.iloc[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_4640\\231902709.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_small['label'] = np.zeros(len(df_small))\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_4640\\231902709.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_doh_small['label'] = np.ones(len(df_doh_small))\n"
     ]
    }
   ],
   "source": [
    "df_small['label'] = np.zeros(len(df_small))\n",
    "df_doh_small['label'] = np.ones(len(df_doh_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index                         128\n",
      "udps.n_bytes_per_packet    859864\n",
      "label                       80000\n",
      "dtype: int64\n",
      "Index                          128\n",
      "udps.n_bytes_per_packet    3741656\n",
      "label                        80000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data, data_doh = df_small, df_doh_small\n",
    "print(data.memory_usage(index=True, deep=True))\n",
    "print(data_doh.memory_usage(index=True, deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.concat([data.sample(frac=1), data_doh.sample(frac=1)]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_data = pd.concat(full_data['udps.n_bytes_per_packet'].apply(lambda x: pd.Series(x)), full_data.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "testData = copy.copy(full_data)\n",
    "testData = testData['udps.n_bytes_per_packet'].apply(lambda x: pd.Series(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en conjunto de entrenamiento y prueba (85% entrenamiento, 15% prueba)\n",
    "train_data, test_data = train_test_split(full_data, test_size=0.15, random_state=42)\n",
    "\n",
    "# Dividir el conjunto de entrenamiento en conjunto de entrenamiento y validación (70% entrenamiento, 15% validación)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las características (X) y etiquetas (y) para cada conjunto de datos\n",
    "X_train = train_data.iloc[:, :-1]  # Todas las columnas excepto la última\n",
    "y_train = train_data.iloc[:, -1]  # Última columna (etiquetas)\n",
    "\n",
    "X_test = test_data.iloc[:, :-1]\n",
    "y_test = test_data.iloc[:, -1]\n",
    "\n",
    "X_val = val_data.iloc[:, :-1]\n",
    "y_val = val_data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de X_train: (14450, 1)\n",
      "Dimensiones de y_train: (14450,)\n",
      "Dimensiones de X_test: (3000, 1)\n",
      "Dimensiones de y_test: (3000,)\n",
      "Dimensiones de X_val: (2550, 1)\n",
      "Dimensiones de y_val: (2550,)\n"
     ]
    }
   ],
   "source": [
    "# Verificar las dimensiones de los conjuntos resultantes\n",
    "print(\"Dimensiones de X_train:\", X_train.shape)\n",
    "print(\"Dimensiones de y_train:\", y_train.shape)\n",
    "print(\"Dimensiones de X_test:\", X_test.shape)\n",
    "print(\"Dimensiones de y_test:\", y_test.shape)\n",
    "print(\"Dimensiones de X_val:\", X_val.shape)\n",
    "print(\"Dimensiones de y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Pandas DataFrame are not supported: apply X.values when calling fit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m clf \u001b[39m=\u001b[39m TabNetClassifier()  \u001b[39m#TabNetRegressor()\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m clf\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      3\u001b[0m   X_train, y_train,\n\u001b[0;32m      4\u001b[0m   eval_set\u001b[39m=\u001b[39;49m[(X_val, y_val)]\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      6\u001b[0m preds \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\black-box\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:200\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[1;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn \u001b[39m=\u001b[39m loss_fn\n\u001b[1;32m--> 200\u001b[0m check_input(X_train)\n\u001b[0;32m    201\u001b[0m check_warm_start(warm_start, from_unsupervised)\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_fit_params(\n\u001b[0;32m    204\u001b[0m     X_train,\n\u001b[0;32m    205\u001b[0m     y_train,\n\u001b[0;32m    206\u001b[0m     eval_set,\n\u001b[0;32m    207\u001b[0m     weights,\n\u001b[0;32m    208\u001b[0m )\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\black-box\\lib\\site-packages\\pytorch_tabnet\\utils.py:351\u001b[0m, in \u001b[0;36mcheck_input\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(X, (pd\u001b[39m.\u001b[39mDataFrame, pd\u001b[39m.\u001b[39mSeries)):\n\u001b[0;32m    350\u001b[0m     err_message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPandas DataFrame are not supported: apply X.values when calling fit\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 351\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(err_message)\n\u001b[0;32m    352\u001b[0m check_array(X)\n",
      "\u001b[1;31mTypeError\u001b[0m: Pandas DataFrame are not supported: apply X.values when calling fit"
     ]
    }
   ],
   "source": [
    "clf = TabNetClassifier()  #TabNetRegressor()\n",
    "clf.fit(\n",
    "  X_train, y_train,\n",
    "  eval_set=[(X_val, y_val)]\n",
    ")\n",
    "preds = clf.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
