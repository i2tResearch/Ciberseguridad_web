{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import pickle\n",
    "import swifter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask import compute, delayed\n",
    "import multiprocessing\n",
    "\n",
    "from model import DeepMALRawPackets\n",
    "from sklearn.metrics import confusion_matrix,  ConfusionMatrixDisplay\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.WARN)  # or any {DEBUG, INFO, WARN, ERROR, FATAL}\n",
    "\n",
    "# Early stopping & checkpoints\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Obtener la ruta de la carpeta donde están los archivos .csv\n",
    "# carpeta_benign = './Dataset/Benign'\n",
    "# # Obtener una lista de todos los archivos .csv en la carpeta\n",
    "# benign_csv_list = [archivo for archivo in os.listdir(carpeta_benign) if archivo.endswith('.csv')]\n",
    "\n",
    "# # Leer los archivos .csv y concatenarlos en un solo dataframe\n",
    "# df = pd.concat([pd.read_csv(os.path.join(carpeta_benign, archivo)) for archivo in benign_csv_list])\n",
    "\n",
    "# #Reiniciar los indices del DataFrame unificado\n",
    "# df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la ruta de la carpeta donde están los archivos .csv\n",
    "# carpeta_benign = './Dataset/Benign'\n",
    "\n",
    "# # Obtener una lista de todos los archivos .csv en la carpeta\n",
    "# benign_csv_list = [archivo for archivo in os.listdir(carpeta_benign) if archivo.endswith('.csv')]\n",
    "\n",
    "# # Obtener la ruta de la carpeta donde se guardarán los archivos pickle transformados\n",
    "# carpeta_transformados = './Dataset/TransformadosBenign'\n",
    "\n",
    "# # Crear la carpeta si no existe\n",
    "# if not os.path.exists(carpeta_transformados):\n",
    "#     os.makedirs(carpeta_transformados)\n",
    "\n",
    "# # Leer los archivos .csv y aplicar la transformación a cada uno\n",
    "# # df_list = []\n",
    "# for archivo in benign_csv_list:\n",
    "#     df = pd.read_csv(os.path.join(carpeta_benign, archivo))\n",
    "#     df['udps.n_bytes_per_packet'] = df['udps.n_bytes_per_packet'].swifter.apply(eval)\n",
    "#     # df_list.append(df)\n",
    "#     # Guardar el DataFrame transformado como un archivo pickle en la carpeta de transformados\n",
    "#     nombre_archivo = os.path.splitext(archivo)[0] + '_transformado.pickle'\n",
    "#     ruta_archivo = os.path.join(carpeta_transformados, nombre_archivo)\n",
    "#     with open(ruta_archivo, 'wb') as f:\n",
    "#         pickle.dump(df, f)\n",
    "\n",
    "# # Concatenar los DataFrames en un solo DataFrame\n",
    "# df = pd.concat(df_list)\n",
    "\n",
    "# # Reiniciar los índices del DataFrame unificado\n",
    "# df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la ruta de la carpeta donde están los archivos .csv\n",
    "# carpeta_malware = './Dataset/Malware'\n",
    "\n",
    "# # Obtener una lista de todos los archivos .csv en la carpeta\n",
    "# malware_csv_list = [archivo for archivo in os.listdir(carpeta_malware) if archivo.endswith('.csv')]\n",
    "\n",
    "# # Obtener la ruta de la carpeta donde se guardarán los archivos pickle transformados\n",
    "# carpeta_transformados = './Dataset/TransformadosMalware'\n",
    "\n",
    "# # Crear la carpeta si no existe\n",
    "# if not os.path.exists(carpeta_transformados):\n",
    "#     os.makedirs(carpeta_transformados)\n",
    "\n",
    "# # Leer los archivos .csv y aplicar la transformación a cada uno\n",
    "# # df_list = []\n",
    "# for archivo in malware_csv_list:\n",
    "#     df_doh = pd.read_csv(os.path.join(carpeta_malware, archivo))\n",
    "#     df_doh = df_doh.iloc[:5000]\n",
    "#     df_doh['udps.n_bytes_per_packet'] = df_doh['udps.n_bytes_per_packet'].swifter.apply(eval)\n",
    "#     # df_list.append(df)\n",
    "#     # Guardar el DataFrame transformado como un archivo pickle en la carpeta de transformados\n",
    "#     nombre_archivo = os.path.splitext(archivo)[0] + '_transformado.pickle'\n",
    "#     ruta_archivo = os.path.join(carpeta_transformados, nombre_archivo)\n",
    "#     with open(ruta_archivo, 'wb') as f:\n",
    "#         pickle.dump(df_doh, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la ruta de la carpeta donde están los archivos .csv\n",
    "carpeta_benign = './Dataset/Benign'\n",
    "# Obtener una lista de todos los archivos .csv en la carpeta\n",
    "benign_csv_list = [archivo for archivo in os.listdir(carpeta_benign) if archivo.endswith('.csv')]\n",
    "\n",
    "# Leer los archivos .csv y concatenarlos en un solo dataframe\n",
    "df = pd.concat([pd.read_csv(os.path.join(carpeta_benign, archivo)) for archivo in benign_csv_list])\n",
    "\n",
    "#Reiniciar los indices del DataFrame unificado\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la ruta de la carpeta donde están los archivos .csv\n",
    "carpeta_malware = './Dataset/Malware'\n",
    "# Obtener una lista de todos los archivos .csv en la carpeta\n",
    "malware_csv_list = [archivo for archivo in os.listdir(carpeta_malware) if archivo.endswith('.csv')]\n",
    "\n",
    "# Leer los archivos .csv y concatenarlos en un solo dataframe\n",
    "df_doh = pd.concat([pd.read_csv(os.path.join(carpeta_malware, archivo)) for archivo in malware_csv_list])\n",
    "\n",
    "#Reiniciar los indices del DataFrame unificado\n",
    "df_doh = df_doh.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Importar datos benignos #\n",
    "# carpeta_transformados_benign = './Dataset/TransformadosBenign'\n",
    "\n",
    "# # Obtener la lista de archivos pickle en la carpeta\n",
    "# pickle_files = [f for f in os.listdir(carpeta_transformados_benign) if f.endswith('.pickle')]\n",
    "\n",
    "# # Crear una lista vacía para almacenar los dataframes cargados\n",
    "# df_list = []\n",
    "\n",
    "# # Cargar cada archivo pickle en la lista\n",
    "# for file in pickle_files:\n",
    "#     with open(os.path.join(carpeta_transformados_benign, file), 'rb') as f:\n",
    "#         df = pickle.load(f)\n",
    "#         df_list.append(df)\n",
    "\n",
    "# # Combinar los dataframes cargados en un solo dataframe\n",
    "# df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Importar datos malignos #\n",
    "# carpeta_transformados_malware = './Dataset/TransformadosMalware'\n",
    "\n",
    "# # Obtener la lista de archivos pickle en la carpeta\n",
    "# pickle_files = [f for f in os.listdir(carpeta_transformados_malware) if f.endswith('.pickle')]\n",
    "\n",
    "# # Crear una lista vacía para almacenar los dataframes cargados\n",
    "# df_doh_list = []\n",
    "\n",
    "# # Cargar cada archivo pickle en la lista\n",
    "# for file in pickle_files:\n",
    "#     with open(os.path.join(carpeta_transformados_malware, file), 'rb') as f:\n",
    "#         df_doh = pickle.load(f)\n",
    "#         df_doh_list.append(df_doh)\n",
    "\n",
    "# # Combinar los dataframes cargados en un solo dataframe\n",
    "# df_doh = pd.concat(df_doh_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign data shape: (309887, 1)\n",
      "Malware data shape: (179252, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Benign data shape: \" + str(df.shape))\n",
    "print(\"Malware data shape: \" + str(df_doh.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df['udps.n_bytes_per_packet'].isnull().sum())\n",
    "#print(df['udps.n_bytes_per_packet'].isna().sum())\n",
    "\n",
    "#print(df_doh['udps.n_bytes_per_packet'].isnull().sum())\n",
    "#print(df_doh['udps.n_bytes_per_packet'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una lista de índices de filas que contienen un error de clave\n",
    "error_indices = []\n",
    "for idx in df_doh.index:\n",
    "    if (len(df_doh.loc[idx]['udps.n_bytes_per_packet']) > 1000000):\n",
    "        error_indices.append(idx)\n",
    "\n",
    "# print(len(error_indices))\n",
    "# Eliminar las filas que contienen un error de clave\n",
    "df_doh = df_doh.drop(error_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doh = df_doh.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut data\n",
    "# df = df.iloc[:186000]\n",
    "# df_doh = df_doh.iloc[:108000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut data\n",
    "df = df.iloc[:int(df.shape[0] * 0.4)]\n",
    "df_doh = df_doh.iloc[:int(df_doh.shape[0] * 0.32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf02f77a8fbc44f8a941b5d440103924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/92966 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['udps.n_bytes_per_packet'] = df['udps.n_bytes_per_packet'].swifter.apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939a213dbfd14a6b8d26f4314d3fbea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/71478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_doh['udps.n_bytes_per_packet'] = df_doh['udps.n_bytes_per_packet'].swifter.apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = np.zeros(len(df))\n",
    "df_doh['label'] = np.ones(len(df_doh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index                          128\n",
      "udps.n_bytes_per_packet    7063152\n",
      "label                       743728\n",
      "dtype: int64\n",
      "Index                          128\n",
      "udps.n_bytes_per_packet    9800672\n",
      "label                       571824\n",
      "dtype: int64\n",
      "Model: \"DeepMAL-Packets\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1024, 1)]         0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 1020, 32)          192       \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 1016, 64)          10304     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 127, 64)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 127, 200)          212000    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25400)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200)               5080200   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 200)               40200     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,343,097\n",
      "Trainable params: 5,343,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data, data_doh = df, df_doh\n",
    "print(data.memory_usage(index=True, deep=True))\n",
    "print(data_doh.memory_usage(index=True, deep=True))\n",
    "\n",
    "deep_mal_classifier = DeepMALRawPackets()\n",
    "print(deep_mal_classifier.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Status] Training model...\n",
      "Index                             128\n",
      "udps.n_bytes_per_packet    8937598400\n",
      "label                         8677280\n",
      "dtype: int64\n",
      "Amount of instances 1084660\n",
      "Amount of positive instances: 759562\n",
      "Epoch 1/10\n",
      "33896/33896 - 21483s - loss: 0.1423 - binary_accuracy: 0.8067 - recall: 0.9970 - precision: 0.7850 - 21483s/epoch - 634ms/step\n",
      "Epoch 2/10\n",
      "33896/33896 - 23002s - loss: 0.1436 - binary_accuracy: 0.8068 - recall: 0.9972 - precision: 0.7850 - 23002s/epoch - 679ms/step\n",
      "Epoch 3/10\n",
      "33896/33896 - 26089s - loss: 0.1409 - binary_accuracy: 0.8091 - recall: 0.9977 - precision: 0.7868 - 26089s/epoch - 770ms/step\n",
      "Epoch 4/10\n",
      "33896/33896 - 26059s - loss: 0.1453 - binary_accuracy: 0.8098 - recall: 0.9977 - precision: 0.7874 - 26059s/epoch - 769ms/step\n",
      "Epoch 5/10\n",
      "33896/33896 - 25572s - loss: 0.1495 - binary_accuracy: 0.8088 - recall: 0.9974 - precision: 0.7867 - 25572s/epoch - 754ms/step\n",
      "Epoch 6/10\n",
      "33896/33896 - 25468s - loss: 0.1467 - binary_accuracy: 0.8036 - recall: 0.9962 - precision: 0.7826 - 25468s/epoch - 751ms/step\n",
      "Epoch 7/10\n",
      "33896/33896 - 26410s - loss: 0.1427 - binary_accuracy: 0.8075 - recall: 0.9972 - precision: 0.7856 - 26410s/epoch - 779ms/step\n",
      "Epoch 8/10\n",
      "33896/33896 - 26923s - loss: 0.1457 - binary_accuracy: 0.8059 - recall: 0.9968 - precision: 0.7844 - 26923s/epoch - 794ms/step\n",
      "Epoch 9/10\n",
      "33896/33896 - 27406s - loss: 0.1453 - binary_accuracy: 0.8045 - recall: 0.9967 - precision: 0.7832 - 27406s/epoch - 809ms/step\n",
      "Epoch 10/10\n",
      "33896/33896 - 29170s - loss: 0.1413 - binary_accuracy: 0.8086 - recall: 0.9974 - precision: 0.7865 - 29170s/epoch - 861ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x227420d4e50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "print('[Status] Training model...')\n",
    "train_data = pd.concat([data.sample(frac=1), data_doh.sample(frac=1)]).sample(frac=1)\n",
    "train_data = train_data.explode('udps.n_bytes_per_packet', ignore_index=True)\n",
    "print(train_data.memory_usage(index=True, deep=True))\n",
    "x_train = list(train_data['udps.n_bytes_per_packet'])\n",
    "y_train = list(train_data['label'])\n",
    "print('Amount of instances', len(train_data.index))\n",
    "print('Amount of positive instances:', len(train_data[train_data['label']==1].index))\n",
    "\n",
    "earlyStop = EarlyStopping(monitor='binary_accuracy', patience=7)\n",
    "model_checkpoint = ModelCheckpoint(filepath='deepmal-{epoch:02d}-{binary_accuracy:.2f}.h5', monitor='binary_accuracy', verbose=0, save_best_only=True, save_freq=\"epoch\")\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "deep_mal_classifier.model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, class_weight={0: 0.5, 1: 0.5}, verbose=2, callbacks=[earlyStop, model_checkpoint]) # {0: 0.012, 1: 0.988}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Status] Predicting...\n",
      "16944/16944 [==============================] - 5827s 344ms/step\n",
      "[480657.] 542180\n",
      "[[ 61137 101315]\n",
      " [   386 379342]]\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "print('[Status] Predicting...')\n",
    "test_data  = pd.concat([data.sample(frac=0.5), data_doh.sample(frac=0.5)]).sample(frac=1)\n",
    "test_data = test_data.explode('udps.n_bytes_per_packet', ignore_index=True)\n",
    "x_test = list(test_data['udps.n_bytes_per_packet'])\n",
    "y_test = list(test_data['label'])\n",
    "predictions = deep_mal_classifier.model.predict(x_test)\n",
    "predictions[predictions>0.5] = 1\n",
    "predictions[predictions<=0.5] = 0\n",
    "print(sum(predictions), len(predictions))\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar el modelo entrenado\n",
    "deep_mal_classifier.model.save(\"deep_mal_v8.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
