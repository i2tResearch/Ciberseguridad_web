{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluacion de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El siguiente script hace la evaluacion de un conjunto de imagenes haciendo uso de un modelo. A continuacion se importan las librerias necesarias para su ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "import numpy as np #operaciones matriciales y con vectores\n",
    "import pandas as pd #tratamiento de datos\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import time\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores de entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La variable test_folder indica la carpeta en donde se encuentran las imagenes que se quieren a evaluar, esta carpeta debe tener dos subcarpetas una Fake y otra Real que corresponden a la naturaleza de la imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La variable model hace referencia al modelo previamente entrenado que hace el proceso de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = '/home/icesi/Documents/DeepFakeFinal/Experimento_8/'\n",
    "model = '/home/icesi/Documents/DeepFakeFinal/Experimento_7_Xception_Rostros_FineTune/Model/model_fine_final.h5'\n",
    "classes = 'classes.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "partial_filenames = [] \n",
    "backup_batch_size = 500\n",
    "\n",
    "for dirname, _, filenames in os.walk(test_folder + 'Fake/'):\n",
    "    pbar = tqdm(total=len(filenames))\n",
    "    for filename in filenames:\n",
    "        command_result = !python inference.py {model} {classes} {os.path.join(dirname, filename)} --top_n 2\n",
    "        probability = command_result[len(command_result)-1]\n",
    "        partial_filenames.append(filename)\n",
    "        predictions.append(probability) \n",
    "        if(len(partial_filenames) % backup_batch_size == 0):\n",
    "            result_fake_df = pd.DataFrame({\"filename\": partial_filenames, \"probability\": predictions})\n",
    "            result_fake_df.to_csv(test_folder+\"result_fake.csv\", index=False)\n",
    "        pbar.update(1)  \n",
    "        \n",
    "result_fake_df = pd.DataFrame({\"filename\": filenames, \"probability\": predictions})\n",
    "result_fake_df.to_csv(test_folder+\"result_fake.csv\", index=False)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "predictions = []\n",
    "partial_filenames = [] \n",
    "backup_batch_size = 500\n",
    "for dirname, _, filenames in os.walk(test_folder + 'Real/'):\n",
    "    pbar = tqdm(total=len(filenames))\n",
    "    for filename in filenames:\n",
    "        command_result = !python inference.py {model} {classes} {os.path.join(dirname, filename)} --top_n 2\n",
    "        probability = command_result[len(command_result)-1]\n",
    "        partial_filenames.append(filename)\n",
    "        predictions.append(probability)\n",
    "        if(len(partial_filenames) % backup_batch_size == 0):\n",
    "            result_fake_df = pd.DataFrame({\"filename\": partial_filenames, \"probability\": predictions})\n",
    "            result_fake_df.to_csv(test_folder+\"result_real.csv\", index=False)\n",
    "        pbar.update(1)  \n",
    "        \n",
    "result_fake_df = pd.DataFrame({\"filename\": filenames, \"probability\": predictions})\n",
    "result_fake_df.to_csv(test_folder+\"result_real.csv\", index=False)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En esta seccíon se generan las metricas de evaluación de acuerdo a la evaluacion de cada imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fake = pd.read_csv(test_folder+\"result_fake.csv\", sep=\",\")\n",
    "data_real = pd.read_csv(test_folder+\"result_real.csv\", sep=\",\")\n",
    "data_fake['ImageReal']=1\n",
    "data_real['ImageReal']=0\n",
    "data=[data_real,data_fake]\n",
    "data=pd.concat(data)\n",
    "data['Result']=0\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization():\n",
    "    i=1\n",
    "    value=0.00\n",
    "    bestI=1\n",
    "    while(i<100):\n",
    "        data[\"Result\"] = np.where((data[\"probability\"]>i),1, data[\"Result\"])\n",
    "        data[\"Result\"] = np.where((data[\"probability\"]<=i),0, data[\"Result\"])\n",
    "        y_true = np.array(data['ImageReal'])\n",
    "        y_scores = np.array(data['Result'])\n",
    "        v=roc_auc_score(y_true, y_scores)\n",
    "        if(value<v):\n",
    "            value=v\n",
    "            bestI=i\n",
    "        i+=1\n",
    "    return bestI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice=optimization()\n",
    "data[\"Result\"] = np.where((data[\"probability\"]>indice),1, data[\"Result\"])\n",
    "data[\"Result\"] = np.where((data[\"probability\"]<=indice),0, data[\"Result\"])\n",
    "data.info()\n",
    "print(\"Best Indice = \"+str(indice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(data['ImageReal'])\n",
    "y_scores = np.array(data['Result'])\n",
    "roc_auc_score(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix :')\n",
    "print(confusion_matrix(y_true, y_scores))\n",
    "print ('Accuracy Score :',accuracy_score(y_true, y_scores) )\n",
    "print( 'Report : ')\n",
    "print (classification_report(y_true, y_scores) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
