import os
import re
import numpy as np
import nltk
nltk.download('stopwords')
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
import json
import requests
from joblib import load

URL = "http://127.0.0.1:4000/path"

response = requests.get(URL)
print(response)

if response.status_code == 200:
    data = response.json()
    print(data)
else:
    print("Something went wrong")


def analize_code(data):
    archive = open(data)
    lines = archive.readlines()
    archive.close()

    words = []
    words_clean = []
    for line in lines:
        result = re.sub(r'\/\*[\s\S]*?\*\/|([^\\:]|^)\/\/.*$', ' ', line)
        result = re.sub(r'\W', ' ', result)
        result = re.sub(r'\s+', ' ', result, flags=re.I)
        words.append(result)

        for word in words:
            if word != "" and word != " ":
                if word not in words_clean:
                    words_clean.append(word)

    n = 15
    fragments = [words_clean[i:i + n] for i in range (0, len(words_clean), n)]
    print("--> FRAGMENTOS DE CODIGO")
    print(fragments)

    frag = []

    for fragment in fragments:
        tfidfconverter = TfidfVectorizer(max_features=2000, stop_words=stopwords.words('english'))  
        frag = tfidfconverter.fit_transform(fragment).toarray()

    return frag


converted_data= analize_code(data)

model = load('classifier.pkl')

# Use the loaded model to make predictions
predictions = model.predict(converted_data)