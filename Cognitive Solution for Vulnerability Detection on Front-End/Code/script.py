import os
import re
import numpy as np
import pandas as pd
import nltk
nltk.download('stopwords')
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
import json
import time
import requests
from joblib import load

URL = "http://127.0.0.1:4000/path"

response = requests.get(URL)

if response.status_code == 200:
    data = response.json()
else:
    print("Something went wrong")


def analize_code(data):
    lines_array= pd.array([data], dtype='string')
    words = []
    words_clean = []
    for line in range(0, len(lines_array)):
        result = re.sub(r'\/\*[\s\S]*?\*\/|([^\\:]|^)\/\/.*$', ' ', str(lines_array[line]))
        result = re.sub(r'\W', ' ', result)
        result = re.sub(r'\s+', ' ', result, flags=re.I)
        words.append(result)

    frag = []
    tfidfconverter = TfidfVectorizer(max_features=208, stop_words=stopwords.words('english'))  
    frag = tfidfconverter.fit_transform(words).toarray()
    
    return frag


converted_data= analize_code(data)

model = load('classifier.pkl')

# Use the loaded model to make predictions
predictions = model.predict(converted_data)

prediction_labels = {
    0: 'Cross-site Scripting',
    1: 'SQL Injection',
    2: 'Insufficient Information'
}

def convert_prediction_to_text(prediction):
  return prediction_labels[prediction]

def run_and_send():
    result= convert_prediction_to_text(predictions[0])

    predictions_response= requests.post("http://127.0.0.1:4000/data", json={'predictions': result})
    send= predictions_response.json()
    print(send)

try:
    while True:
        run_and_send()
        time.sleep(60)
except KeyboardInterrupt:
    print("Exiting...")
    exit()